import numpy as np
import matplotlib.pyplot as plt
import random

from sklearn import metrics
from sklearn import svm
from sklearn import datasets


#1.训练svm，标记所有未标记数据
#2.计算欧式距离，分为两个类，一个计算两个数据的欧式距离，一个用来找出距离随机点最近的n个点(用map和数组实现)
#3.用knn预测边缘节点的标记
#4.将预测完的边缘节点放到训练数据中，迭代，直到无未标记数据



def load_data():
    '''
    加载数据集
    '''
    #digits = datasets.load_digits()
    ######   混洗样本　########
    #rng = np.random.RandomState(0)
    #indices = np.arange(len(digits.data))  # 样本下标集合
    #rng.shuffle(indices)  # 混洗样本下标集合
    #X = digits.data[indices] #所有数据
    #y = digits.target[indices] #所有标记
    all = [[6,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],
          [3,1,0,1,0,0,1,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],
          [15,2,0,3,0,0,4,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,2,1,0,1,0,1,1,1,1,1,0,1,1,0,1,2,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],
          [6,1,0,1,0,0,1,0,0,2,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,2,1,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],
          [5,0,0,3,0,0,2,1,0,3,0,0,0,0,0,1,1,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],
          [7,2,0,2,0,0,3,0,0,4,1,0,0,3,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],
          [3,1,0,1,0,0,2,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,0,1,0,1,1,1,1,1,0,1,1,0,1,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],
          [3,1,0,1,0,0,2,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,0,1,0,1,1,1,1,1,0,1,1,0,1,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],
          [4,1,0,1,0,0,2,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,0,1,0,1,1,1,1,1,0,1,1,0,1,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],
          [4,1,0,1,0,0,1,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,0,1,0,0,1,1,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],
          [5,1,0,1,0,0,1,0,0,2,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,1,0,1,0,1,1,1,1,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],
          [11,2,0,2,0,0,4,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,1,1,0,1,0,1,1,1,1,1,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],
          [10,1,0,1,0,0,3,0,0,0,1,0,0,0,0,0,3,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,2,2,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],
          [10,1,0,1,0,0,3,0,0,0,1,0,0,0,0,0,3,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,2,2,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],
          [11,2,0,2,0,0,4,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,1,1,0,1,0,1,1,1,1,1,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],
          [4,1,0,1,0,0,2,0,0,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,1,0,1,1,1,1,1,0,1,1,0,1,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],
          [5,1,0,1,0,0,2,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,0,1,0,1,1,1,1,1,0,1,1,0,1,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],
          [8,1,0,1,0,0,2,0,0,2,0,0,0,2,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,1,1,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],
          [5,1,0,1,0,0,1,0,0,2,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,2,1,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],
          [4,1,0,1,0,0,2,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,0,1,0,1,1,1,1,1,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],
          [6,1,0,1,0,0,2,0,0,2,1,0,0,2,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,1,1,1,1,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],
          [20,1,2,5,0,0,6,0,0,10,0,0,0,1,0,3,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],
          [4,1,0,1,0,0,2,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,0,1,0,1,1,1,0,1,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],
          [7,1,0,1,0,0,1,0,0,1,0,0,0,0,0,1,1,0,0,0,0,0,0,1,0,0,0,0,0,2,1,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],
          [5,1,0,1,0,0,2,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],
          [7,1,0,2,0,0,2,1,0,3,1,0,0,2,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,1,1,1,1,1,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],
          [8,1,0,1,0,0,2,0,0,2,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],
          [11,2,0,2,0,0,2,0,0,4,0,0,0,0,0,1,1,0,0,0,0,0,0,1,0,0,0,0,1,2,1,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],
          [14,1,1,2,0,0,5,1,1,3,0,0,0,1,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],
          [10,0,0,2,0,0,2,2,0,1,2,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,1],
          [3,1,0,2,0,0,2,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,1,1,0,1,0,1,1,1,2,2,0,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],
          [9,2,0,2,0,0,4,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,2,1,0,1,0,1,1,1,1,1,1,1,1,0,1,2,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],
          [8,0,0,4,0,0,1,4,0,4,1,0,0,2,0,2,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1],
          [5,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],
          [11,2,0,2,0,0,2,0,0,4,0,0,0,0,0,2,1,0,0,0,0,0,0,1,0,0,0,0,1,2,1,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],
          [7,1,0,1,0,0,1,0,0,1,0,0,0,0,0,1,1,0,0,0,0,0,0,1,0,0,0,0,0,2,1,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,0,0,1,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],
          [4,0,0,2,1,0,2,1,0,4,0,0,0,1,0,0,0,0,0,0,0,0,0,4,0,0,0,0,0,0,1,0,1,0,1,0,0,1,3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3,0,0,0,0,0,0,0,0,0,0,0,0,1],
          [3,0,0,1,0,0,5,0,0,2,0,1,0,1,0,1,1,1,1,0,0,0,0,4,0,0,0,0,0,0,0,0,0,0,0,0,0,1,3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],
          [4,0,0,1,0,0,1,1,0,2,0,0,0,1,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,1,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,1,0,0,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1],
          [18,3,0,2,0,0,5,1,0,3,1,0,0,1,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0],
          [7,0,0,2,0,0,2,1,0,3,1,0,0,0,0,0,1,1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,1,2,1,0,0,0,0,0,0,0,0,0,0,0,0,1],
          [10,0,1,2,0,0,7,0,0,2,0,0,0,1,0,1,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,5,6,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,3,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0],
          [1,0,0,0,0,0,4,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,3,1,0,0,0,0,0,0,0,0,0,0,0,0,1,5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,1,1,0,0,0,0,0,0],
          [1,0,0,0,0,0,3,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,4,0,0,0,0,0,0,0,0,0,0,0,0,0,1,4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,0,0,0,0,0,0],
          [1,0,0,1,0,0,6,1,1,0,0,0,0,0,0,3,0,0,3,2,1,0,0,2,0,0,0,0,0,0,0,0,0,0,1,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],
          [12,0,0,2,0,0,0,2,0,3,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,1,0,0,0,0,0,0,0,2,0,0,0,0,1],
          [5,0,0,1,0,0,1,0,0,4,0,0,0,2,1,1,1,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1],
          [3,0,0,2,0,0,4,0,1,2,0,0,0,1,0,3,0,0,0,0,0,0,0,2,1,0,0,0,0,0,0,0,0,0,0,0,0,1,4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,2,0,0,0,1,1,0,1,0,0,0,0,0,0,0,1],
          [16,2,0,2,0,0,6,2,0,10,0,0,0,5,0,0,0,0,0,0,0,0,0,5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3,3,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,1],
          [2,0,0,1,0,0,0,1,0,1,0,0,0,0,0,2,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1],
          [6,2,0,1,0,0,2,1,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,1,1,0,0,0,0,0,0,1,0,1,1,1,0],
          [4,0,0,1,0,0,1,1,0,1,0,0,0,1,0,1,0,0,0,0,0,0,1,2,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],
          [22,1,0,2,0,0,1,1,0,3,0,0,0,3,0,1,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0],
          [32,2,0,3,0,1,8,2,0,3,2,1,0,0,0,0,0,0,2,0,0,0,0,4,0,0,0,0,0,0,0,0,0,0,0,0,0,3,3,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,1,1,0,0,0,1,1,0,1,1,1,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],
          [43,2,0,0,0,0,7,0,0,0,0,0,0,4,0,3,0,0,0,0,0,0,0,5,0,0,0,0,0,0,0,0,0,0,0,0,0,5,5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3,3,0,1,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],
          [10,0,0,0,0,0,4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,2,0,0,0,0,0,0,0,0,0,0,0,0,1,5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,1,0,0,0,0,0,0,1],
          [3,0,0,2,0,0,6,0,0,1,0,1,0,0,0,1,0,1,4,1,0,0,0,3,0,0,0,0,0,0,0,0,0,0,1,0,0,0,3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3,0,0,0,0,0,0],
          [14,3,0,3,0,0,5,0,0,8,1,0,0,1,0,0,0,0,0,0,0,0,0,2,0,0,1,1,0,1,1,0,1,0,1,0,0,0,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],
          [23,3,0,3,0,0,5,0,0,5,0,0,0,5,0,0,3,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,1,3,3,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],
          [19,1,0,1,0,0,5,3,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,1,1],
          [19,0,0,0,0,0,0,0,0,0,0,0,0,6,0,0,8,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],
          [11,1,0,1,0,0,3,1,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],
          [4,0,0,1,0,0,1,1,0,1,1,0,0,2,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0],
          [9,0,0,3,0,0,2,2,1,1,0,0,0,0,0,1,0,0,2,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,1,0,1,0,0,0],
          [12,0,0,1,0,0,2,3,0,0,1,0,0,3,0,0,2,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,1,1,0]]

    random.shuffle(all)
    all = np.array(all)
    # print(all)
    yAll = all[:, 108]
    # print(yAll)
    XAll = np.delete(all, 108, axis=1)  # 删除最后一列
    # print(XAll)
    X = XAll[:39]  # 取得总数据的前39行
    XPredict = XAll[39:]  # 获得后26条数据
    y = yAll[:39]  # 取得前39行的标记
    yTrue = yAll[39:]  # 获取后26条数据的标记
    # print(len(y))

    #混洗样本,再次混洗，可不加
    #X = np.insert(X, 108, values=y, axis=1)
    #random.shuffle(X)
    #X = np.array(X)
    #y = X[:, 108]
    #print(y)
    #X = np.delete(X, 108, axis=1)  # 删除最后一列

    ###### 生成未标记样本的下标集合 ####
    # 只有 40% 的样本有标记
    #n_labeled_points = 16
    XLabel = X[:16] #获取前16条数据为标记数据
    yLabel = y[:16] #获取前16条数据的标记
    #print(n_labeled_points)
    # 后面 60% 的样本未标记
    #unlabeled_indices = np.arange(len(y))[n_labeled_points:]
    XUnlabel = X[16:] #将后23条数据作为未标记数据
    #print(unlabeled_indices)
    return XLabel, yLabel, XUnlabel, XPredict, yTrue


# def semi_predict(*data):
#     XLabel, yLabel, XPredict, yTrue = data
#     clf = svm.SVC(gamma='scale', decision_function_shape='ovo')
#     clf.fit(XLabel, yLabel)
#     yPredict = clf.predict(XPredict)
#     print("Accuracy:%f" % metrics.accuracy_score(yTrue, yPredict))
#     return yPredict

# 半监督学习semi-SVM
def test_semi_SVM(XLabel, yLabel, XUnlabel):
    '''
    测试 LabelPropagation 的用法
    '''
    #XLabel, yLabel, XUnlabel = data
    #print("get ytrue")
    #print(yTrue)
    XAllLabel = np.insert(XLabel, 108, values=yLabel, axis=1)
    random.shuffle(XAllLabel)
    XAllLabel = np.array(XAllLabel)
    yLabel = XAllLabel[:, 108]
    #print(yAll)
    XLabel = np.delete(XAllLabel, 108, axis=1)  # 删除最后一列

    # 必须拷贝，后面要用到 y
    XTrain1 = XLabel
    XTrain2 = XLabel[:round(len(XLabel)/2 + 0.5)] #将训练数据对半分
    XTrain3 = XLabel[round(len(XLabel)/2 + 0.5):]

    yTrain1 = yLabel
    yTrain2 = yLabel[:round(len(XLabel)/2 + 0.5)] #将训练数据标记对半分
    yTrain3 = yLabel[round(len(XLabel)/2 + 0.5):]

    clf1 = svm.SVC(gamma='scale', decision_function_shape='ovo')
    clf1.fit(XTrain1, yTrain1)
    yPredSVM1 = clf1.predict(XUnlabel)
    clf2 = svm.SVC(gamma='scale', decision_function_shape='ovo')
    clf2.fit(XTrain2, yTrain2)
    yPredSVM2 = clf2.predict(XUnlabel)
    clf3 = svm.SVC(gamma='scale', decision_function_shape='ovo')
    clf3.fit(XTrain3, yTrain3)
    yPredSVM3 = clf3.predict(XUnlabel)
    #count = 0;
    for i in range(0,len(yPredSVM1))[::-1]:
        if(yPredSVM2[i] == yPredSVM3[i]):
            if(yPredSVM1[i] == yPredSVM2[i]):
                pass
            else:
                yPredSVM1[i] = yPredSVM3[i]
        else:
            pass
    return yPredSVM1

    ### 获取预测准确率
    # 预测标记
    #predicted_labels = clf.predict(XPredict)
    #print(XPredict)
    #predicted_labels = clf.transduction_[unlabeled_indices]
    # 真实标记
    #yTrue
    #true_labels = y[unlabeled_indices]
    #print("Accuracy:%f" % metrics.accuracy_score(yTrue, predicted_labels))
    # 或者 print("Accuracy:%f"%clf.score(X[unlabeled_indices],true_labels))


class KNearestNeighbor:
    """ a kNN classifier with L2 distance """

    def __init__(self):  # self相当于java中的this
        pass  # 空语句，为了保持程序的结构

    def train(self, X, y):
        """
        Train the classifier. For k-nearest neighbors this is just
        memorizing the training data.

        Input:
        X - A num_train x dimension array where each row is a training point.
        y - A vector of length num_train, where y[i] is the label for X[i, :]
        """
        self.X_train = X
        self.y_train = y

    def predict(self, X, k, num_loops):  # k=1, num_loops=0
        """
        Predict labels for test data using this classifier.

        Input:
        X - A num_test x dimension array where each row is a test point.
        k - The number of nearest neighbors that vote for predicted label
        num_loops - Determines which method to use to compute distances
                    between training points and test points.

        Output:
        y - A vector of length num_test, where y[i] is the predicted label for the
            test point X[i, :].
        """
        if num_loops == 0:
            dists = self.compute_distances_no_loops(X)
        elif num_loops == 1:
            dists = self.compute_distances_one_loop(X)
        elif num_loops == 2:
            dists = self.compute_distances_two_loops(X)
        else:
            raise ValueError('Invalid value %d for num_loops' % num_loops)

        return self.predict_labels(dists, k=k)

    def compute_distances_two_loops(self, X):
        """
        Compute the distance between each test point in X and each training point
        in self.X_train using a nested loop over both the training data and the
        test data.

        Input:
        X - An num_test x dimension array where each row is a test point.

        Output:
        dists - A num_test x num_train array where dists[i, j] is the distance
                between the ith test point and the jth training point.
        """
        num_test = X.shape[0]
        num_train = self.X_train.shape[0]
        dists = np.zeros((num_test, num_train))
        for i in range(num_test):  # 原来是xrange,xrange和range的用法一样
            for j in range(num_train):
                #####################################################################
                # TODO:                                                             #
                # Compute the l2 distance between the ith test point and the jth    #
                # training point, and store the result in dists[i, j]               #
                #####################################################################

                dists[i, j] = np.sqrt(np.sum(np.square(X[i, :] - self.X_train[j, :])))

                pass
                #####################################################################
                #                       END OF YOUR CODE                            #
                #####################################################################
        return dists

    def compute_distances_one_loop(self, X):
        """
        Compute the distance between each test point in X and each training point
        in self.X_train using a single loop over the test data.

        Input / Output: Same as compute_distances_two_loops
        """
        num_test = X.shape[0]
        num_train = self.X_train.shape[0]
        dists = np.zeros((num_test, num_train))
        for i in range(num_test):
            #######################################################################
            # TODO:                                                               #
            # Compute the l2 distance between the ith test point and all training #
            # points, and store the result in dists[i, :].                        #
            #######################################################################

            dists[i, :] = np.sqrt(np.sum(np.square(self.X_train - X[i, :]), axis=1))

            #######################################################################
            #                         END OF YOUR CODE                            #
            #######################################################################
        return dists

    def compute_distances_no_loops(self, X):
        """
        Compute the distance between each test point in X and each training point
        in self.X_train using no explicit loops.

        Input / Output: Same as compute_distances_two_loops
        """
        num_test = X.shape[0]
        num_train = self.X_train.shape[0]
        dists = np.zeros((num_test, num_train))
        #########################################################################
        # TODO:                                                                 #
        # Compute the l2 distance between all test points and all training      #
        # points without using any explicit loops, and store the result in      #
        # dists.                                                                #
        # HINT: Try to formulate the l2 distance using matrix multiplication    #
        #       and two broadcast sums.                                         #
        #########################################################################

        train_sums = np.sum(np.square(self.X_train), axis=1)  # sum over each row #a^2

        test_sums = np.sum(np.square(X), axis=1)  # sum over each row #b^2

        dists = (train_sums - 2.0 * np.dot(X, self.X_train.T)).T + test_sums  # (a- b)^2 = a2 + b2 - 2ab

        dists = np.sqrt(dists.T)

        pass
        #########################################################################
        #                         END OF YOUR CODE                              #
        #########################################################################
        return dists

    def predict_labels(self, dists, k=1):
        """
        Given a matrix of distances between test points and training points,
        predict a label for each test point.

        Input:
        dists - A num_test x num_train array where dists[i, j] gives the distance
                between the ith test point and the jth training point.

        Output:
        y - A vector of length num_test where y[i] is the predicted label for the
            ith test point.
        """
        num_test = dists.shape[0]
        y_pred = np.zeros(num_test)
        for i in range(num_test):
            # A list of length k storing the labels of the k nearest neighbors to
            # the ith test point.
            closest_y = []
            #########################################################################
            # TODO:                                                                 #
            # Use the distance matrix to find the k nearest neighbors of the ith    #
            # training point, and use self.y_train to find the labels of these      #
            # neighbors. Store these labels in closest_y.                           #
            # Hint: Look up the function numpy.argsort.                             #
            #########################################################################

            indices = np.argsort(dists[i, :])[:k]  # k indices corresponding to the distance

            for idx in indices:  # for every index, append the label to closest_y

                closest_y.append(self.y_train[idx])

            #########################################################################
            # TODO:                                                                 #
            # Now that you have found the labels of the k nearest neighbors, you    #
            # need to find the most common label in the list closest_y of labels.   #
            # Store this label in y_pred[i]. Break ties by choosing the smaller     #
            # label.                                                                #
            #########################################################################

            y_pred[i] = np.argmax(np.bincount(closest_y))

            pass
            #########################################################################
            #                           END OF YOUR CODE                            #
            #########################################################################
        print(y_pred)

        return y_pred

def svm_knn(XLabel, yLabel, XUnlabel):
    yPredict = test_semi_SVM(XLabel, yLabel, XUnlabel)
    #XLabel, yLabel, XUnlabel, XPredict, yTrue = data #XLabel, yLabel 分别为标记数据和标记,XUnlabel 为未受标记的数据, XPredict 预测的数据, yTrue 预测数据的标记

    #融合Unlabel和yPredict
    XAllPredict = np.insert(XUnlabel, 108, values=yPredict, axis=1)
    print(XAllPredict)
    #将结果为1和0分开，并混洗
    sum0 = 0;
    sum1 = 0;
    for i in range(0, len(XAllPredict)): #计算标记为0和1的数据数
        if(XAllPredict[i, 108] == 0):
            sum0 = sum0 + 1
        else:
            sum1 = sum1 + 1
    for i in range(0, sum0):
        if(XAllPredict[i, 108] == 1):
            for j in range(sum0, XAllPredict.shape[0]):
                if(XAllPredict[j, 108] == 0):
                    XAllPredict[[i,j],:] = XAllPredict[[j,i],:]
    all0 = XAllPredict[:sum0]  # 获取前sum条数据为标记数据
    print(all0)
    all1 = XAllPredict[sum0:]  # 将后面的数据作为未标记数据
    print(all1)
    # lielength = int(XAllPredict.shape[1])
    # all0 = np.zeros([sum, int(lielength)], dtype = int) #创建填充为0的数组，行为标记为0的数目，列为数组列的长度
    # all1 = np.zeros([sum1, int(lielength)], dtype = int)
    # n = 0
    # m = 0
    # for i in range(0, XAllPredict.shape[0]): #将标记结果为0和1的分开
    #     if(XAllPredict[i, 108] == 0):
    #         for j in range(0, XAllPredict.shape[1]):
    #             all0[n, j] == XAllPredict[i, j]
    #         n = n + 1
    #     else:
    #         for j in range(0, XAllPredict.shape[1]):
    #             all1[m, j] == XAllPredict[i, j]
    #         m = m + 1
    allX0 = np.delete(all0, 108, axis=1)  # 删除最后一列
    allX1 = np.delete(all1, 108, axis=1)  # 删除最后一列

    random.shuffle(allX0)
    random.shuffle(allX1)

    #取第一个，计算欧式距离，排序，取前n小的,当unlabel的数据为空时返回
    #计算欧式距离 distanc = np.linalg.norm(allX0[0] - allX1[i]) distanc = np.linalg.norm(allX1[0] - allX0[i])
    distance0 = np.zeros(len(allX0), dtype=float) #随机取一个标记为1的到所有标记为0的欧式距离
    distance1 = np.zeros(len(allX1), dtype=float)

    XKnnPredict = np.zeros([6, XAllPredict.shape[1]], dtype=int)
    if((len(allX0)) >= 3 and len(allX1) >= 3):
        for i in range(0,len(allX0)):
            distance0[i] = np.linalg.norm(allX1[0] - allX0[i])
        for i in range(0,len(allX1)):
            distance1[i] = np.linalg.norm(allX0[0] - allX1[i])
        #print("distance0 = " + distance0)
        #print("distance1 = " + distance1)
        min01 = find_Min(distance0) #将前三小的数取出来,1到0的距离,diatance0
        distance0[min01] = 100
        min02 = find_Min(distance0)
        distance0[min02] = 100
        min03 = find_Min(distance0)
        for j in range(0, allX0.shape[1]): #将最小的三条复制出来
            XKnnPredict[0, j] = allX0[min01, j]
            XKnnPredict[1, j] = allX0[min02, j]
            XKnnPredict[2, j] = allX0[min03, j]
        for i in range(0,len(allX0))[::-1]:
            if((i == min01) or (i == min02) or (i == min03)):
                allX0 = np.delete(allX0, i, axis=0)

        min04 = find_Min(distance1)  # 将前三小的数取出来,0到1的距离,distance1
        distance1[min04] = 100
        min05 = find_Min(distance1)
        distance1[min05] = 100
        min06 = find_Min(distance1)
        for j in range(0, allX1.shape[1]): #将最小的三条复制出来
            XKnnPredict[3, j] = allX1[min04, j]
            XKnnPredict[4, j] = allX1[min05, j]
            XKnnPredict[5, j] = allX1[min06, j]
        for i in range(0,len(allX1))[::-1]:
            if((i == min04) or (i == min05) or (i == min06)):
                allX1 = np.delete(allX1, i, axis=0)


        XUnlabel = np.vstack((allX0, allX1))

        myKnn = KNearestNeighbor()
        myKnn.train(XLabel, yLabel)
        predicted_labels = myKnn.predict(XKnnPredict, 1, 0)
        #XKnnPredict = np.insert(XKnnPredict, 108, values=predicted_labels, axis=1)
        XLabel = np.vstack((XLabel, XKnnPredict))
        #yLabel = np.vstack((yPredict, predicted_labels))
        #yLabel.extend(predicted_labels)
        a = np.zeros(len(yLabel) + len(predicted_labels), dtype=int)
        a = np.array(a)
        #print(a)
        sum = 0
        for i in range(0, len(yLabel)):
            a[sum] = yLabel[i]
            sum = sum + 1
        for i in range(0, len(predicted_labels)):
            a[sum] = yPredict[i]
            sum = sum + 1
        svm_knn(XLabel, a, XUnlabel)

    else:
        XLabel = np.vstack((XLabel, XUnlabel))
        #yLabel = np.vstack((yLabel, yPredict))
        #yPredict = np.array(yPredict)
        #print("yLabel = " + yLabel)
        #print("yPredict = " + yPredict)
        #yLabel = yLabel + yPredict
        #yLabel = np.vstack((yLabel, yPredict))
        #yLabel.extend(yPredict)
        #yLabel.extend(yPredict)
        a = np.zeros(len(yLabel) + len(yPredict), dtype=int)
        a = np.array(a)
        # print(a)
        sum = 0
        for i in range(0, len(yLabel)):
            a[sum] = yLabel[i]
            sum = sum + 1
        for i in range(0, len(yPredict)):
            a[sum] = yPredict[i]
            sum = sum + 1
        return XLabel, a


def find_Min(ary_list):
    min_index = 0
    #min_value = ary_list[0]
    for i in range(1,len(ary_list)):
        if(ary_list[i] < ary_list[min_index]):
            min_index = i
    return min_index

def svm_predict(*data):
    X, y, XPredict = data
    clf = svm.SVC(gamma='scale', decision_function_shape='ovo')
    clf.fit(X, y)
    predicted_labels = clf.predict(XPredict)
    return predicted_labels

if __name__ == '__main__':
    data = load_data()
    XLabel, yLabel, XUnlabel, XPredict, yTrue = data
    #print("XLabel = " + XLabel)


    #data = XLabel, yLabel, XUnlabel
    data1 = svm_knn(XLabel, yLabel, XUnlabel)
    XLabel1, yLabel1 = data1

    data2 = XLabel1, yLabel1, XPredict
    predicted_labels = svm_predict(*data2)

    print("Accuracy:%f" % metrics.accuracy_score(yTrue, predicted_labels))


# 获取半监督分类数据集
#data = load_data()
# 调用 test_LabelPropagation
#test_semi_SVM(*data)









